{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting transformers\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n\u001b[K     |████████████████████████████████| 573kB 8.2MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.22.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers) (1.16.4)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.10.2)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from transformers) (0.1.83)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.0.12)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.36.1)\nRequirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers) (0.7)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2019.8.19)\nCollecting sacremoses\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n\u001b[K     |████████████████████████████████| 890kB 23.4MB/s eta 0:00:01\n\u001b[?25hCollecting tokenizers==0.5.2\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n\u001b[K     |████████████████████████████████| 3.7MB 34.3MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2019.9.11)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.8)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.24.2)\nRequirement already satisfied: botocore<1.14.0,>=1.13.2 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (1.13.2)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (0.9.4)\nRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (0.2.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.12.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (0.13.2)\nRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.2->boto3->transformers) (0.15.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.2->boto3->transformers) (2.8.0)\nBuilding wheels for collected packages: sacremoses\n  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=c423bbfce62d5cbbb13407a19181df262520d2e4c8182a7fb8f31d3b8cbe79e3\n  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\nSuccessfully built sacremoses\nInstalling collected packages: sacremoses, tokenizers, transformers\nSuccessfully installed sacremoses-0.0.41 tokenizers-0.5.2 transformers-2.8.0\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"We'll be using the Transformers package by HuggingFace that provides the model architectures and pre-trained weights to the latest Transformer models including XLNet, T5, and many others.\n\nLet's import the relevant libraries and load the data into a Pandas Dataframe"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, random_split, DataLoader\nfrom transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm, tqdm_notebook\n\nif torch.cuda.is_available:\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\n# Load data into dataframe\ndf = pd.read_json('../input/news-category-dataset/News_Category_Dataset_v2.json', lines=True)\nprint(df.shape)\ndf.head()","execution_count":2,"outputs":[{"output_type":"stream","text":"(200853, 6)\n","name":"stdout"},{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"        category                                           headline  \\\n0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n\n           authors                                               link  \\\n0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n2       Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n3       Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n4       Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n\n                                   short_description       date  \n0  She left her husband. He killed their children... 2018-05-26  \n1                           Of course it has a song. 2018-05-26  \n2  The actor and his longtime girlfriend Anna Ebe... 2018-05-26  \n3  The actor gives Dems an ass-kicking for not fi... 2018-05-26  \n4  The \"Dietland\" actress said using the bags is ... 2018-05-26  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category</th>\n      <th>headline</th>\n      <th>authors</th>\n      <th>link</th>\n      <th>short_description</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CRIME</td>\n      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n      <td>Melissa Jeltsen</td>\n      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n      <td>She left her husband. He killed their children...</td>\n      <td>2018-05-26</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENTERTAINMENT</td>\n      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n      <td>Andy McDonald</td>\n      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n      <td>Of course it has a song.</td>\n      <td>2018-05-26</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ENTERTAINMENT</td>\n      <td>Hugh Grant Marries For The First Time At Age 57</td>\n      <td>Ron Dicker</td>\n      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n      <td>2018-05-26</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ENTERTAINMENT</td>\n      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n      <td>Ron Dicker</td>\n      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n      <td>The actor gives Dems an ass-kicking for not fi...</td>\n      <td>2018-05-26</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENTERTAINMENT</td>\n      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n      <td>Ron Dicker</td>\n      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n      <td>The \"Dietland\" actress said using the bags is ...</td>\n      <td>2018-05-26</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop rows without description and merge title with description\ndf = df.loc[df.loc[:, 'short_description']!=\"\"]\ndf['text'] = df.apply(lambda x: x['headline']+\". \"+x[\"short_description\"], axis=1)\nprint(\"Average text length: {}\".format(np.mean(df['text'].str.len())))\n\n# Helper function to tokenize our textual data\ndef convert_lines(input_sentences,tokenizer, max_seq_length=200):\n    max_seq_length -=2\n    all_tokens = []\n    for text in tqdm(input_sentences):\n        # Tokenizes the individual sentences\n        tokens = tokenizer.tokenize(text)\n        if len(tokens)>max_seq_length:\n            # Cuts the sentence if it is longer than the maximum length\n            tokens = tokens[:max_seq_length]\n        \n        # Add tokens to indicate the start and end of the sentence, and pads the sentence with 0\n        padded_tokens = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens))\n        all_tokens.append(padded_tokens)\n    return np.array(all_tokens)\n\n# Instantiate Tokenizer and tokenize all our data\ntokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\nx = convert_lines(df['text'], tokenizer, 200)\n\n# Getting the categories and number of labels\ncategories = df['category'].unique().tolist()\nnum_labels = len(categories)\nprint(categories)\nprint(\"Number of categories: {}\".format(num_labels))\n\n# Creating an index for each unique category and assigning corresponding category index to each news article\ncat2id = {cat:i for i,cat in enumerate(categories)}\nid2cat = {i:cat for cat,i in cat2id.items()}\nprint(cat2id)\ndf['cat_id'] = df.apply(lambda x: cat2id[x['category']], axis=1)\nlabels = df['cat_id'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting train-validation-test split size and batch size\ntest_portion = 0.2\nval_portion = 0.2\nbatch_size = 12\n\n# Loading data and defining dataloaders\ndataset = TensorDataset(torch.from_numpy(x).long(), torch.tensor(labels,dtype=torch.long))\ntest_data_len = int(test_portion*len(dataset))\nval_data_len = int(val_portion*len(dataset))\ntrain_data_len = len(dataset) - test_data_len - val_data_len\ntrain_data, val_data, test_data = random_split(dataset, [train_data_len, val_data_len, test_data_len])\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting training Hyperparameters\nEPOCHS = 3\naccumulation_steps = 2\nlr = 1e-5\n\n# Instantiating our model with the the last layer as a classification layer and output dimension = number of labels\nmodel = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\",num_labels=num_labels)\nmodel.zero_grad()\nmodel.to(device)\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n]\n\nnum_train_optimization_steps = int(EPOCHS*len(train_data)/batch_size/accumulation_steps)\nnum_warmup_steps = int(0.05 * num_train_optimization_steps)\n\noptimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_optimization_steps)\nmodel.train()\nmodel.logits_proj","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_model_file = \"../working/xlnet.pt\"\nlowest_val_acc = 0\n\n# Training Loop\ntq = tqdm_notebook(range(EPOCHS)) # Main loop progress bar\nfor epoch in tq:\n    model.train()\n    avg_loss = 0.\n    avg_accuracy = 0.\n    lossf=None\n    # Progress bar for iterating through training data\n    progress_train = tqdm_notebook(enumerate(train_loader),total=len(train_loader),leave=False)\n    optimizer.zero_grad()\n    for i,(x_batch, y_batch) in progress_train:\n        y_pred = model(x_batch.to(device), attention_mask=(np.logical_not(x_batch==0)).to(device), labels=None)\n        # Calculate loss function\n        loss =  F.cross_entropy(y_pred[0],y_batch.to(device))\n        loss.backward()\n        if (i+1) % accumulation_steps == 0:\n            # Update model weights\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n        if lossf:\n            lossf = 0.98*lossf+0.02*loss.item()\n        else:\n            lossf = loss.item()\n        progress_train.set_postfix(loss = lossf)\n        avg_loss += loss.item() / len(train_loader)\n        avg_accuracy += torch.mean((torch.max(F.softmax(y_pred[0], dim=1), dim=1)[1] == y_batch.to(device)).to(torch.float)).item()/len(train_loader)\n    tq.set_postfix(avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n    print(\"Training Accuracy: {}%\".format(avg_accuracy*100))\n    \n    # Validation\n    model.eval() # Freeze model weights for validation evaluation\n    val_loss = 0.\n    val_acc = 0.\n    # Progress bar for iterating through validation data\n    progress_val = tqdm_notebook(enumerate(val_loader), total=len(val_loader), leave=False)\n    for i, (val_x, val_y) in progress_val:\n        val_pred = model(val_x.to(device), attention_mask=(np.logical_not(val_x==0)).to(device), labels=None)\n        val_loss += F.cross_entropy(val_pred[0],val_y.to(device)).item()/len(val_loader)\n        val_acc += torch.mean((torch.max(F.softmax(val_pred[0], dim=1), dim=1)[1] == val_y.to(device)).to(torch.float)).item() / len(val_loader)\n    print(\"Validation Loss: {}\".format(val_loss))\n    print(\"Validation Accuracy: {}%\".format(val_acc*100))\n    if val_acc > lowest_val_acc:\n        # Save model if validation accuracy is higher for current loop\n        torch.save(model.state_dict(), output_model_file)\n        lowest_val_acc = val_acc\n        print(\"Validation performance improved... Saving model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the best model\noutput_model_file = \"../working/xlnet.pt\"\nmodel.load_state_dict(torch.load(output_model_file, map_location=device))\nmodel.eval()\nprogress_test = tqdm_notebook(test_loader)\ntest_loss = 0.\ntest_acc = 0.\nfor i,(x_batch, y_batch)  in enumerate(progress_test):\n    y_pred = model(x_batch.to(device), attention_mask=(np.logical_not(x_batch==0)).to(device), labels=None)\n    test_loss += F.cross_entropy(y_pred[0], y_batch.to(device)).item()/len(test_loader)\n    test_acc += torch.mean((torch.max(F.softmax(y_pred[0], dim=1), dim=1)[1] == y_batch.to(device)).to(torch.float)).item()/len(test_loader)\nprint(\"Test Loss: {}\".format(test_loss))\nprint(\"Test accuracy: {}%\".format(test_acc*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}